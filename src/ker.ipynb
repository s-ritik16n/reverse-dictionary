{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import losses\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_found(array, val):\n",
    "    flag = False\n",
    "    for arr in array:\n",
    "        for a in arr:\n",
    "            if a != val:\n",
    "                print(a)\n",
    "                print(\"found\")\n",
    "                flag = True\n",
    "                break\n",
    "        if flag:\n",
    "            break\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "found\n",
      "1\n",
      "found\n"
     ]
    }
   ],
   "source": [
    "# with open(\"pkl-db/wh.pkl\",\"rb\") as f:\n",
    "#     whpkl = pickle.load(f)\n",
    "# #     check_if_found(whpkl,0)\n",
    "    \n",
    "# with open(\"pkl-db/bh.pkl\",\"rb\") as f:\n",
    "#     bhpkl = pickle.load(f)\n",
    "# #     check_if_found(bhpkl,0)\n",
    "    \n",
    "# with open(\"pkl-db/wout.pkl\",\"rb\") as f:\n",
    "#     woutpkl = pickle.load(f)\n",
    "# #     check_if_found(woutpkl,0)\n",
    "    \n",
    "# with open(\"pkl-db/bout.pkl\",\"rb\") as f:\n",
    "#     boutpkl = pickle.load(f)\n",
    "# #     check_if_found(boutpkl,0)\n",
    "    \n",
    "# with open(\"pkl-db/output.pkl\",\"rb\") as f:\n",
    "#     outputpkl = pickle.load(f)\n",
    "# #     check_if_found(outputpkl,0.5)\n",
    "    \n",
    "with open(\"pkl-db/X.pkl\",\"rb\") as f:\n",
    "    X = pickle.load(f)\n",
    "    check_if_found(X,0)\n",
    "    \n",
    "with open(\"pkl-db/y.pkl\",\"rb\") as f:\n",
    "    y = pickle.load(f)\n",
    "    check_if_found(y,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['abduc', 0, 'especi muscl', '6337 5994'], ['abduc', 0, 'draw away midlin bodi adjac part', '1000 1004 6338 798 588 1027'], ['adduc', 2, 'especi muscl', '6337 5994'], ['adduc', 2, 'bring togeth draw toward midlin bodi toward adjac part', '3593 1034 1000 6575 6338 798 6575 588 1027'], ['moribund', 4, 'point death', '1040 1068'], ['moribund', 4, 'breath last', '354 1076'], ['direct', 6, 'lack compromis mitig element', '1091 1092 6192 1094'], ['direct', 6, 'exact', '1095'], ['rel', 8, 'estim comparison', '6339 1096'], ['rel', 8, 'absolut complet', '669 1099']]\n"
     ]
    }
   ],
   "source": [
    "defs = pd.read_csv(\"db/sample/clean/def_col_vectors.csv\", names=[\"word\", \"word_vector\",\"definition\", \"def_vectors\"], sep=\":\", index_col=None, keep_default_na=False, na_values=[\"\"])\n",
    "word_list = pd.read_csv(\"db/sample/clean/word_col_vectors.csv\", names=[\"word\", \"word_vector\"], sep=\":\", index_col=None, keep_default_na=False, na_values=[\"\"])\n",
    "\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "defs = defs.values.tolist()\n",
    "word_list = word_list.values.tolist()\n",
    "defs_main = defs[:10]\n",
    "\n",
    "defs_main = [[str(d[0]).strip(), d[1], str(d[2]).strip(), str(d[3]).strip()] for d in defs_main]\n",
    "size = len(word_list)\n",
    "print(defs_main)\n",
    "for n in defs_main:\n",
    "    main_word = str(n[0])\n",
    "    definition = str(n[2]).split()\n",
    "    vector = str(n[3]).split()\n",
    "    out = np.zeros((size,), dtype=int)\n",
    "    inp = np.zeros((size,), dtype=int)\n",
    "    for word in word_list:\n",
    "        if word[0] is main_word:\n",
    "            out[word[1]] = 1\n",
    "        if word[0] in definition:\n",
    "            inp[word[1]] = 1\n",
    "    X.append(inp)\n",
    "    y.append(out)\n",
    "\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:10]\n",
    "y = y[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in enumerate(y):\n",
    "    for v in val:\n",
    "        if v != 0:\n",
    "            print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(3,activation='sigmoid', input_dim=size))\n",
    "model.add(Dense(size, activation='sigmoid'))\n",
    "model.compile(optimizer=keras.optimizers.SGD(lr=0.01), loss='mse')\n",
    "model.summary()\n",
    "model.fit(X, y, epochs=100, verbose=1)\n",
    "\n",
    "for yyy in  y:\n",
    "    for yy in yyy:\n",
    "        print(yy)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_revdict)",
   "language": "python",
   "name": "conda_revdict"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
